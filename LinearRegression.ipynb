{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWF3lzsFRBpsKgaBV663ZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leogils333/ML_AI/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "9bkCJJIg4Gr5",
        "outputId": "a7cc501d-ffa9-4cca-d1e7-358893b6427f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope of the Line (w)  [1.28]\n",
            "Y Interceptor (b)  -12.000000000000007\n",
            "New Y Predicted values f(x) = w * X + b  [13.6 20.  26.4 32.8 39.2]\n",
            "Predict Ice sales at 27C  [22.56]\n",
            "Cost function 1.3856406460551005\n",
            "1.3856406460551005 - Cost Error is high or poor so w and b values are not correct/perfect\n",
            "Cost calculation  [0.96]\n",
            "Gradient calculation  (array([6.81984]), array([0.161792]))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to numpy.ndarray.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-86a7641989e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m w_final, b_final, J_hist, p_hist = gradient_descent(X, Y, liregmod_slope, liregmod_intercept, learning_rate, number_of_iteration,\n\u001b[0m\u001b[1;32m     94\u001b[0m                                            cost_calculation, gradient_calculation)\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-86a7641989e7>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, w_in, b_in, alpha, number_of_iteration, cost_function, gradient_function)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mp_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_iteration\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n\u001b[0m\u001b[1;32m     88\u001b[0m                   \u001b[0;34mf\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                   f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
          ]
        }
      ],
      "source": [
        "#NumPy is a powerful library for numerical computing in Python.\n",
        "#NumPy arrays are designed for efficient storage and manipulation of numerical data\n",
        "import math, copy\n",
        "import numpy as npy\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temperature = [20, 25, 30, 35, 40] #Python List\n",
        "icesales = [13, 21, 25, 35, 38]\n",
        "\n",
        "#Finding Linear regression values w and b by using X and Y\n",
        "X = npy.array([temperature]).T #numpy array\n",
        "Y = npy.array(icesales);\n",
        "\n",
        "liregmod = LinearRegression()\n",
        "liregmod = liregmod.fit(X, Y)\n",
        "\n",
        "liregmod_slope = liregmod.coef_\n",
        "liregmod_intercept = liregmod.intercept_\n",
        "print('Slope of the Line (w) ',liregmod_slope)\n",
        "print('Y Interceptor (b) ',liregmod_intercept)\n",
        "\n",
        "Y_predicte = liregmod.predict(X)\n",
        "\n",
        "print('New Y Predicted values f(x) = w * X + b ',Y_predicte)\n",
        "print('Predict Ice sales at 27C ',liregmod.predict([[27]]))\n",
        "\n",
        "#Finding Cost value to see how far the predicted value from the Actual value\n",
        "rmse = npy.sqrt(mean_squared_error(Y, Y_predicte))\n",
        "print('Cost function',rmse)\n",
        "print(rmse, '- Cost Error is high or poor so w and b values are not correct/perfect')\n",
        "\n",
        "#Implementing gradient descent. Need Learning rate to implement GD\n",
        "learning_rate = 0.001\n",
        "number_of_iteration = 1000\n",
        "cost_history = []\n",
        "\n",
        "def cost_calculation(X, Y, w, b):\n",
        "  #1/2m(sum of (Y_hat - y)power of 2)\n",
        "  m = X.shape[0]\n",
        "  cost = 0\n",
        "\n",
        "  for i in range(m):\n",
        "    f_wb = w * X[i] + b\n",
        "    cost = cost + (f_wb - Y[i])**2\n",
        "  total_cost = 1 / (2 * m) * cost\n",
        "  return total_cost\n",
        "\n",
        "print('Cost calculation ',cost_calculation(X, Y, liregmod_slope, liregmod_intercept))\n",
        "\n",
        "def gradient_calculation(X, Y, w, b):\n",
        "  m = X.shape[0]\n",
        "  dj_dw = 0\n",
        "  dj_db = 0\n",
        "\n",
        "  for i in range(m):\n",
        "    f_wb = w * X[i] + b\n",
        "    dj_dw_i = (f_wb - Y[i]) * X[i]\n",
        "    dj_db_i = f_wb - Y[i]\n",
        "    dj_dw = dj_dw + dj_dw_i\n",
        "    dj_db = dj_db + dj_db_i\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "  return dj_dw, dj_db\n",
        "\n",
        "print('Gradient calculation ',gradient_calculation(X, Y, liregmod_slope, liregmod_intercept))\n",
        "\n",
        "J_history = []\n",
        "p_history = []\n",
        "\n",
        "def gradient_descent(X, Y, w_in, b_in, alpha, number_of_iteration, cost_function, gradient_function):\n",
        "  w = w_in\n",
        "  b = b_in\n",
        "\n",
        "  for i in range(number_of_iteration):\n",
        "    dj_dw, dj_db = gradient_function(X, Y, w, b)\n",
        "\n",
        "    w = w - alpha * dj_dw\n",
        "    b = b - alpha * dj_db\n",
        "    cost_history.append(cost_function(X, Y, w, b))\n",
        "    if i< number_of_iteration:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(X, Y, w , b))\n",
        "            p_history.append([w,b])\n",
        "    if i% math.ceil(number_of_iteration/10) == 0:\n",
        "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
        "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
        "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
        "\n",
        "  return w, b, J_history, p_history\n",
        "\n",
        "w_final, b_final, J_hist, p_hist = gradient_descent(X, Y, liregmod_slope, liregmod_intercept, learning_rate, number_of_iteration,\n",
        "                                           cost_calculation, gradient_calculation)\n",
        "\n",
        "print('Gradient Descent ',w_final, b_final, J_hist, p_hist)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
        "ax1.plot(J_hist[:100])\n",
        "ax2.plot(1000 + npy.arange(len(J_hist[1000:])), J_hist[1000:])\n",
        "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
        "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
        "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
        "plt.show()\n",
        "\n",
        "#print(cost_history)\n",
        "#Draw a Diagram\n",
        "plt.title('Ice Sales vs Temperature')\n",
        "plt.xlabel('Temperature')\n",
        "plt.ylabel('Ice Sales')\n",
        "plt.scatter(temperature, icesales, marker='*', edgecolors='r')\n",
        "plt.plot(temperature, Y_predicte, '-bo')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ]
}